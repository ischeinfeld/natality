---
title: 'US Natality Data: Causal Inference Case Study'
output:
  html_document:
    df_print: paged
---

```{r echo=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE, comment=NA)

library(ggplot2)   # plot
library(dplyr)     # tables
library(readr)     # reading
library(stringr)   # strings
library(purrr)     # fp
library(tidyr)     # remove_na
library(forcats)   # factor recode
library(glmnet)    # lasso
library(grf)       # random forests
library(janitor)   # remove_constant columns
library(corrplot)  # plot correlation matrix
#library(sufrep)    # sufficient representations
source("make_encoder.R")

rm(list = ls())    # reset environment
source("utils.R")  # import helpers
```

# Subset and preprocess data

```{r}
year <- 2002
```

To begin with we sample 500,000 birth records uniformly at random and preprocess
their columns types from the raw CSV to encode each variable as an integer or 
an ordered or unordered factor. All values coded as missing or undetermined
are considered NA.

```{r}
subset <- FALSE
subset_size <- as.integer(1e5)
subset_fname <- paste0("../data/subset_rds/natl", year, ".rds")

if (subset) {
  raw_fname <- paste0("../data/raw_csv/natl", year, ".csv")
  natl_raw <- 
    read_csv(raw_fname, col_types = cols(.default = col_character())) %>%
    sample_n(subset_size)

  write_rds(natl_raw, subset_fname, compress = "gz")
}
```

```{r}
preprocess <- FALSE

preprocessed_fname <- paste0("../data/preprocessed_rds/natl", year, ".rds")

if (preprocess) {
  # load preprocess_natl for this year
  source(paste0("preprocess/preprocess_natl", year, ".R"))
  
  natl <- read_rds(subset_fname) %>%
    preprocess_natl()
  
  write_rds(natl, preprocessed_fname, compress = "gz")
}

natl <- read_rds(preprocessed_fname)
```

# Prepare and Explore Data 

The following variables will be considered in our analysis. Since the original
natality dataset contains multiple codings for the same variables, here we
select a single coding per variable of interest. Some other variables have been
dropped as well, and ?/? of original data covariates are listed. 

```{r}
# variables (with most specific coding) available for analysis
natl_names <- list(
  general = c("restatus",   # Resident Status
              "pldel",      # Place or Facility of Birth
              "birattnd"),  # Attendant at Birth
  occur =   c("stnatexp",   # Expanded State of Occurrence
              "cntocpop"),  # Population Size of County of Occurrence
  resid =   c("stresexp",   # Expanded State of Residence
              "cntrspop",   # Population Size of County of Residence
              "citrspop",   # Population Size of City of Residence
              "metrores",   # Metropolitan - Non-metropolitan County of Residence
              "cntrspop"),  # Population Size of County of Residence
  mother =  c("dmage",      # Age of Mother
              "ormoth",     # Hispanic Origin of Mother
              "mrace",      # Race of Mother
              "dmeduc",     # Education of Mother
              "dmar",       # Marital status
              "mplbir",     # Place of Birth of Mother
              "adequacy",   # Adequacy Of Care Recode (Kessner Index)
              "nlbnl",      # Number of Live Births Now Living
              "nlbnd",      # Number of Live Births Now Dead
              "noterm",     # Number of Other Terminations
              "dlivord",    # Detail Live Birth Order (nlbnl + nlbnd + 1)
              "monpre",     # Detail Month of Pregnancy Prenatal Care Began
              "nprevis"),   # Total Number of Prenatal Visits
  father =  c("dfage",      # Age of Father
              "orfath",     # Hispanic Origin of Father
              "frace"),     # Race of Father
  child =   c("dgestat",    # Gestation - Detail in Weeks
              "csex",       # Sex
              "dbirwt",     # Birth Weight - Detail in Grams
              "dplural",    # Plurality
              "fmaps",      # Five Minute Apgar Score
              "delmeth5"),  # Method of Delivery Recode
  med_rsk = c("anemia", "cardiac", "lung", "diabetes", "herpes", "hydra", "hemo",
              "chyper", "phyper", "eclamp", "incervix", "pre4000", "preterm",
              "renal", "rh", "uterine", "othermr"),
  oth_rsk = c("tobacco",    # Tobacco Use During Pregnancy
              "cigar",      # Average Number of Cigarettes Per Day
              "alcohol",    # Alcohol Use During Pregnancy
              "drink",      # Average Number of Drinks Per Week
              "wtgain"),    # Weight Gain
  obstetr = c("amnio", "monitor", "induct", "stimula", "tocol", "ultras", "otherob"),
  lab_cmp = c("febrile", "meconium", "rupture", "abruptio", "preplace", "excebld",
              "seizure", "precip", "prolong", "dysfunc", "breech", "cephalo",
              "cord", "anesthe", "distress", "otherlb"),
  newborn = c("nanemia", "injury", "alcosyn", "hyaline", "meconsyn", "venl30",
              "ven30m", "nseiz", "otherab"),
  congntl = c("anen", "spina", "hydro", "microce", "nervous", "heart", "circul",
              "rectal", "tracheo", "omphalo", "gastro", "genital", "renalage", 
              "urogen", "cleftlp", "adactyly", "clubfoot", "hernia", "musculo"))

natl <- natl %>%
  select(all_of(as.vector(unlist(natl_names))))
```

As a sanity check for our preprocessing and to start exploring the data, we
tabulate some quick summary information.

```{r}
natl %>%
  summarise(across(.fns = function (x) sum(is.na(x)) / length(x))) %>%
  pivot_longer(cols = everything()) %>%
  arrange(desc(value)) %>%
  filter(value > 0.1)
```

The top 4 missing attributes are mplbir (mother's place of birth), fmaps (five 
minute apgar score), cntocpop (population size of county of occurrence), and
wtgain (weight gain).

Note that missing values in this dataset can be due to a variety of factors. A
large proportion of missingness is explained by regional variations in reporting,
leading to the highly structured correlation of missing values.

```{r}
M <- natl %>%
  sample_n(1000) %>%
  summarise(across(.fns = is.na)) %>%
  remove_constant() %>%
  cor()

corrplot(M, method="circle", order="hclust", tl.pos = "n")
```

# Factor representation

Our models will require exclusively numeric-valued inputs, and we therefore need
to encode group-valued variables as numeric. First we split the data into 
variables that can be directly converted to numbers (i.e.numeric values, ordered
factors, and binary factors) from those that require group encoding.

```{r}
natl_num <- natl %>% 
  select_if(function (col) {
    is.numeric(col) || 
    (is.factor(col) && is.ordered(col)) ||
    (is.factor(col) && (nlevels(col) == 2))
  }) %>%
  transmute(across(everything(), as.numeric))

natl_grp <- natl %>%
  select(-one_of(names(natl_num)))

ncol(natl_num)
ncol(natl_grp)
```

One way to encode group membership is using one-hot encodings.

```{r}
natl_grp_onehot <- 
  model.matrix(~., model.frame(~., natl_grp, na.action=na.pass))[,-1] %>% 
  as_tibble()

ncol(natl_grp_onehot)
```

Note that this massively expands the number of variables since some of our
group variables have many levels (i.e. 50+ geographic regions). 

```{r}
natl_grp_nlevels <- sapply(natl_grp, nlevels)
natl_grp_nlevels
```

Most of the one-hot dimensions come from `stnatexp`, `stresexp`, and `mplbir`,
all of which are geographic variables. The race variables `mrace` and `frace`
also contribute many group levels / dimensions. For these variables specifically,
it would be helpful to use alternate encodings.

One alternate class of encoding schemes, sufficient representations, are 
described in (TODO cite). We define different encoding types for the group
variables based on their level counts.

```{r}
natl_grp_encoding_types <- natl_grp_nlevels %>%
  map(function(size) {
    if (size < 10) "one_hot" else "sparse_low_rank"
  })
```

We then generate encoders of these types.

```{r}
source("make_encoder.R")

natl_grp_encoders <- map2(natl_grp_encoding_types, natl_grp,
  function(encoding, G) {
    make_encoder(encoding, X = as.matrix(natl_num), G = G, num_components = 3)
  })
```

And generate the corresponding encodings.

```{r}
natl_grp_encodings <- map2(natl_grp_encoders, natl_grp,
  function(encoder, G) {
    encoder(X = as.matrix(natl_num), G)
  })
```

Finally, 

```{r}
onehot_encoder <- make_encoder(X = natl_num, G = G, method = "one_hot")
```


Now we determine which subsets of variables we would like to consider as
potential treatments, covariates, and outcomes.

```{r}
# select data
Xnames <- with(natl_names, c(general, mother))
Ynames <- with(natl_names, c(child, newborn, congntl, lab_comp))
Wnames <- with(natl_names, oth_risk)
```

# Analyses

## Ignore missing values

First we prepare our data.

```{r}
Yname <- "excebld"
Wname <- "tobacco"

Yname_expanded <- "excebldyes"
Wname_expanded <- "tobaccoyes"

Xname_full <- c(names(Xdata_num), names(Xdata_fct))
Xname_full_expanded <- c(names(Xdata_num), names(Xdata_fct_expanded))

data_full <- bind_cols(
    Ydata_num, Ydata_fct, 
    Wdata_num, Wdata_fct, 
    Xdata_num, Xdata_fct) %>%
  select(all_of(c(Yname, Wname, Xname_full))) %>%
  rename(Y = matches(Yname), W = matches(Wname))

data_full_expanded <- bind_cols(
    Ydata_num, Ydata_fct_expanded, 
    Wdata_num, Wdata_fct_expanded, 
    Xdata_num, Xdata_fct_expanded
  ) %>%
  select(all_of(c(Yname_expanded, Wname_expanded, Xname_full_expanded))) %>%
  rename(Y = matches(Yname_expanded), W = matches(Wname_expanded))

# filter to desired data
data <- data_full %>%
  drop_na()

data_expanded <- data_full_expanded %>%
  drop_na() %>%
  remove_constant(quiet = FALSE) # remove constant columns

# required since data_expanded drops columns
Xname_expanded <- intersect(Xname_full_expanded, names(data_expanded))

print(paste(nrow(data) / subset_size, "of data doesn't contain NAs"))
```

Our treatment and outcome have the following counts:

```{r}
with(data, table(Y, W))
```

Estimating the propensity score using logistic regression and random forest.

```{r results='hide'}
Xmod = as.matrix(data_expanded[,Xname_expanded])
Ymod = data_expanded$Y
Wmod = data_expanded$W
```

```{r}
# Computing the propensity score by logistic regression of W on X.
p_logistic.fit <- cv.glmnet(Xmod, Wmod, family = "binomial", relax=FALSE, trace=TRUE)
```

```{r}
p_logistic <- predict(p_logistic.fit, newx = Xmod, type = "response")[,1]
```

We can examine the resulting coefficients (note variables are unnormalized and
this is not a measure of variable importance).

```{r}
coefs <- coef(p_logistic.fit)
perm <- order(abs(as.vector(coefs)), decreasing = TRUE)
vars <- rownames(coefs)[perm]
coefs <- coefs[perm]

tibble(vars = vars, coefs = coefs) %>%
  head(10)
```

```{r}
plot(p_logistic.fit)
```

```{r message=FALSE}
cal_plot(Wmod, p_logistic, "Propensity Score Calibration, Logistic", bins = 10, lim = c(0,0.1))
```
We can also use a forest to model the propensity score.

```{r}
# Computing the propensity score by logistic regression of W on X.
#p_forest.fit <- regression_forest(Xmod, Wmod, num.trees = 1000)
```

```{r}
#p_forest <- predict(p_forest.fit, Xmod)$predictions
```

```{r message=FALSE}
#cal_plot(Wmod, p_forest, "Propensity Score Calibration, RF", bins = 10, lim = c(0,1))
```

Estimating the ATE using multiple methods. First, for comparison, the raw difference
in means estimator:
  
```{r}
difference_in_means <- function(dataset, Yvar, Wvar) {
  # Filter treatment / control observations, pulls outcome variable as a vector
  y1 <- dataset %>% dplyr::filter(get(Wvar) == 1) %>% dplyr::pull(get(Yvar))
  y0 <- dataset %>% dplyr::filter(get(Wvar) == 0) %>% dplyr::pull(get(Yvar))
  
  n1 <- sum(dataset[,Wvar])     # Number of obs in treatment
  n0 <- sum(1 - dataset[,Wvar]) # Number of obs in control
  
  # Difference in means is ATE
  tauhat <- mean(y1) - mean(y0)
  
  # 95% Confidence intervals
  se_hat <- sqrt( var(y0)/(n0-1) + var(y1)/(n1-1) )
  lower_ci <- tauhat - 1.96 * se_hat
  upper_ci <- tauhat + 1.96 * se_hat
  
  return(c(ATE = tauhat, lower_ci = lower_ci, upper_ci = upper_ci))
}

tauhat_difference_in_means <- difference_in_means(data_expanded, "Y", "W")

tauhat_difference_in_means
```

Regression adjustment.

```{r}
ate_condmean_ols <- function(dataset, Yvar, Wvar) {
  df_mod_centered <- data.frame(scale(dataset, center = TRUE, scale = FALSE))
  
  lm.interact <- lm(as.formula(paste(Yvar, "~ . * ", Wvar)),
                   data = df_mod_centered)
  tau.hat <- as.numeric(coef(lm.interact)[Wvar])
  se.hat <- as.numeric(sqrt(vcovHC(lm.interact)[Wvar, Wvar]))
  
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_ols <- ate_condmean_ols(data_expanded, "Y", "W")
print(tauhat_ols)
```

IPW

```{r}
ipw <- function(dataset, Yvar, Wvar, p) {
  W <- dataset[[Wvar]]
  Y <- dataset[[Yvar]]
  G <- ((W - p) * Y) / (p * (1 - p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_logistic_ipw <- ipw(data_expanded, "Y", "W", p_logistic)
print(tauhat_logistic_ipw)
```

Propensity-weighted OLS.

```{r}
prop_score_ols <- function(dataset, Yvar, Wvar, p) {
  W <- dataset[[Wvar]]
  Y <- dataset[[Yvar]]
  # Computing weights
  weights <- (W / p) + ((1 - W) / (1 - p))
  # OLS
  lm.fit <- lm(Y ~ W, data = dataset, weights = weights)
  tau.hat = as.numeric(coef(lm.fit)["W"])
  se.hat = as.numeric(sqrt(vcovHC(lm.fit)["W", "W"]))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_pscore_ols <- prop_score_ols(data_expanded, "Y", "W", p_logistic)
print(tauhat_pscore_ols)
```

AIPW

```{r}
aipw_ols <- function(dataset, Yvar, Wvar, p) {
  ols.fit <-  lm(as.formula(paste(Yvar, "~ . * ", Wvar)), data = dataset)
  
  dataset.treatall <- dataset
  dataset.treatall <- dataset.treatall %>%
    mutate(!!Wvar := 1)
  treated_pred = predict(ols.fit, dataset.treatall)
  
  dataset.treatnone = dataset
  dataset.treatall <- dataset.treatnone %>%
    mutate(!!Wvar := 0)
  control_pred = predict(ols.fit, dataset.treatnone)
  
  actual_pred = predict(ols.fit, dataset)
  
  G <- treated_pred - control_pred +
    ((dataset[[Wvar]] - p) * (dataset[[Yvar]] - actual_pred)) / (p * (1 - p))
  tau.hat <- mean(G)
  se.hat <- sqrt(var(G) / (length(G) - 1))
  c(ATE=tau.hat, lower_ci = tau.hat - 1.96 * se.hat, upper_ci = tau.hat + 1.96 * se.hat)
}

tauhat_lin_logistic_aipw <- aipw_ols(data_expanded, "Y", "W", p_logistic)
print(tauhat_lin_logistic_aipw)
```

## Missing covariates

```{r}
data <- data_full %>%
  drop_na(Y, W)

data_expanded <- data_full_expanded %>%
  drop_na(Y, W) %>%
  remove_constant(quiet = FALSE) # remove constant columns

# required since data_expanded drops columns
Xname_expanded <- intersect(Xname_full_expanded, names(data_expanded))

print(paste(nrow(data_expanded) / subset_size, "of data doesn't contain NAs"))

# subsample for speed
data_expanded <- data_expanded %>%
  sample_n(as.integer(1e5))
```

```{r results='hide'}
Xmod = as.matrix(data_expanded[,Xname_expanded])
Ymod = data_expanded$Y
Wmod = data_expanded$W

na_fracs <- colMeans(is.na(data))
perm <- order(abs(as.vector(na_fracs)), decreasing = TRUE)
vars <- names(na_fracs)[perm]
na_fracs <- na_fracs[perm]

tibble(vars = vars, na_fracs = na_fracs) %>%
  head(5)
```

Causal forest.

```{r}
tau.forest <- causal_forest(Xmod, Ymod, Wmod)
```

```{r}
average_treatment_effect(
  tau.forest,
  target.sample = "all",
  method = "AIPW",
  subset = NULL
)
```

```{r}
average_treatment_effect(
  tau.forest,
  target.sample = "treated",
  method = "AIPW",
  subset = NULL
)
```

```{r}
tau.hat <- predict(tau.forest)$predictions
hist(tau.hat)
```

```{r}
importance <- variable_importance(tau.forest)
perm <- order(as.vector(importance), decreasing = TRUE)
vars <- colnames(Xmod)[perm]
importance <- importance[perm]

tibble(vars = vars, importance = importance) %>%
  head(10)
```
